---
title: "Report FIFA 2019 DataBase"
author: "LoÃ¯c Clerc"
date: "17 juin 2019"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
  ```{r- set seed to 1, message=FALSE, echo=FALSE, warning=FALSE}
#In order to have consistent results everytime, the seed is set to 1:
  set.seed(1)
```

```{r loading-libs, message=FALSE, echo=FALSE, warning=FALSE}
  if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
  if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
  if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")
  if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
  if(!require(matrixStats)) install.packages("matrixStats", repos = "http://cran.us.r-project.org")
  if(!require(Rborist)) install.packages("Rborist", repos = "http://cran.us.r-project.org")
  if(!require(gam)) install.packages("gam", repos = "http://cran.us.r-project.org")
  if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org")
```

```{r loading-data, message=FALSE, echo=FALSE}
  dat_raw <- read.csv("https://raw.githubusercontent.com/loicclerc/FIFA-2019-Project/master/data.csv", stringsAsFactors = FALSE)
```

```{r data-Wranglin, message=FALSE, echo=FALSE}
  #Converting columns containing caracters into numbers. It is necessary to the scaling variable into account 
  #to multiply by the right factor
    dat_tuned <- dat_raw %>% mutate(Last_Char_Value = substr(dat_raw$Value, nchar(dat_raw$Value), nchar(dat_raw$Value)), 
            Factor_Value = case_when(Last_Char_Value=="M" ~ 10^6,
                                     Last_Char_Value=="K" ~ 10^3,
                                     TRUE ~ 0),
            Value = case_when(Last_Char_Value==0 ~ 0,
                              TRUE ~ as.numeric(substr(dat_raw$Value, 4, nchar(dat_raw$Value)-1)) * 
                                Factor_Value),
            Last_Char_Wage = substr(dat_raw$Wage, nchar(dat_raw$Wage), nchar(dat_raw$Wage)), 
            Factor_Wage = case_when(Last_Char_Wage=="K" ~ 10^3,
                                    TRUE ~ 0),
            Wage = case_when(Last_Char_Wage==0 ~ 0,
                             TRUE ~ as.numeric(substr(dat_raw$Wage, 4, nchar(dat_raw$Wage)-1)) * 
                               Factor_Wage),
            Last_Char_Release.Clause = substr(dat_raw$Release.Clause, nchar(dat_raw$Release.Clause), 
                                              nchar(dat_raw$Release.Clause)),
            Factor_Release.Clause = case_when(Last_Char_Release.Clause=="M" ~ 10^6,
                                              Last_Char_Release.Clause=="K" ~ 10^3,
                                              TRUE ~ 0),
            Release.Clause = case_when(Last_Char_Release.Clause=="" ~ 0,
                                       TRUE ~ as.numeric(substr(dat_raw$Release.Clause, 4, nchar(dat_raw$Release.Clause)-1)) * 
                                         Factor_Release.Clause)) %>%

  #Selection of the predictors that will be used to train the Database
    select(Age, Overall, Potential, Value, Wage, International.Reputation, 
           Weak.Foot, Skill.Moves, Crossing, Finishing, 
           HeadingAccuracy, ShortPassing, Volleys, Dribbling, Curve, FKAccuracy, 
           LongPassing, BallControl, Acceleration, SprintSpeed, Agility, Reactions, 
           Balance, ShotPower, Jumping, Stamina, Strength, LongShots, Aggression,
           Interceptions, Positioning, Vision, Penalties, Composure, Marking, StandingTackle, 
           SlidingTackle, GKDiving, GKHandling, GKKicking, GKPositioning, GKReflexes, Release.Clause)

  #Renaming problematic column names
    names(dat_tuned)[names(dat_tuned) == "International.Reputation"] <- "International_Reputation"
    names(dat_tuned)[names(dat_tuned) == "Skill.Moves"] <- "Skill_Moves"
    names(dat_tuned)[names(dat_tuned) == "Release.Clause"] <- "Release_Clause"
    
  #Replacing the na's by 0 (seems more meanignful as column average here)
    dat_tuned[is.na(dat_tuned)] <- 0
    
```



## Introduction

In this project, we are analyzing the FIFA 2019 player Data set. This data set provides a lot of information - name, age, Nationality, potential, wage, skills, etc.- 89 columns overall -  on about 18'000 players. Here is a first idea of what the database looks like for a select range of columns:


```{r-data base, echo=FALSE}
dat_raw %>% select(Name, Age, Nationality, Potential, Value, Wage, International.Reputation, Finishing, HeadingAccuracy) %>% head()
# jpeg - [https://github.com/loicclerc/FIFA-2019-Project/Capture_FIFA_Database.JPEG](https://github.com/loicclerc/FIFA-2019-Project/Capture_FIFA_Database.JPEG)
```


Note that the quite aweful character chains in column "Value" and "Wage" are the encoding for the "Euro" sign. This will obviously require some data wragling. The database may be found and downloaded from the Kaggle website (it is required to create an account) or from my Github directory "FIFA-2019-Project" (file "data.csv"):

<https://www.kaggle.com/karangadiya/fifa19> 

<https://github.com/loicclerc/FIFA-2019-Project>

After reflexion, it seemed quite clear that being able to evaluate the value of a player in the market (column value of above picture) is very useful. Indeed, taking the role of a professional team, it would be very practical to have a tool that would evaluate what a player's value is, either in order to set up his price during a transfer or to compare it to the value of a real offer that could be made by another team in order to see if the deal seems good or not. 

Exploring a little bit the distribution of the values, it is clear that the task will be quite challenging as the range of different values is very large, with quite a lot of outlayers:

```{r - Value distrib - unscaled, message=FALSE, echo=FALSE, fig.align='center'}
dat_tuned%>%group_by(Value)%>%
  ggplot(aes(Value)) +
  geom_histogram(color = "black") + ylab("number of players") + ggtitle("players distribution by value") + theme(plot.title = element_text(hjust=0.5))
```

Indeed, we see that the values range from 0 to ~100M Euro with very few players when we go over 40M Euro. We get a better visualization when looking at the same graphic with a log-2 scale:

```{r - Value distrib - scaled log, message=FALSE, echo=FALSE, fig.align='center'}
Value_without_0 <- dat_tuned$Value
Value_without_0[Value_without_0==0] <- 1
dat_tuned%>% mutate(Value_without_0=Value_without_0) %>% group_by(Value_without_0)%>%
  ggplot(aes(Value_without_0)) +
  geom_histogram(color = "black") + scale_x_continuous(trans="log2") + xlab("Value")+ ylab("number of players") + ggtitle("players distribution by value - log scale")+ theme(plot.title = element_text(hjust=0.5))
```

We see that some player have basically a value of 0 while most of them are worth between 100K Euro and 1M Euro. Exploring the bottom of the list, we see that a non neglectable number of players are considered having no or a very low value:


```{r - top 10 low values, message=FALSE, echo=FALSE, fig.align='center'}
dat_tuned%>%group_by(Value)%>% summarize(n_player=n()) %>% arrange(Value)
```


At the other end of the list, three players are worth more than a million Euro:


```{r - top 10 high values, message=FALSE, echo=FALSE, fig.align='center'}
dat_tuned%>%group_by(Value)%>% summarize(n_player=n()) %>% arrange(desc(Value))
```


In this project, attempts will be made to build a model to evaluate the values of the players from the other data available. Thus, the method followed will be regression.

The key steps performed are the following:

1. Loading the data and data wrangling to convert the characters and to obtain workable numericals

2. Data Visualization and pre-processing of the data, especially scalling of the predictors

3. Model testing and optimization

4. Analysis of the results

5. Conclusion


## Method and modelling approach

Before anything else, the first step to perform is to wrangle the data in order to obtain workable numerical data for the columns that are considered as relevant for the model but that contain caracters that must be removed. They are mostly due to two things, the "Euro" sign and the suffix K and M. Difficulty here is that there are signs to remove before and after the actual numbers to extract and also that depending of the coefficents "K" or "M" (or if there is no coefficient) there would be a factor to apply to the results. For instance, for the column "Value", the below table summarizes the number of occurences in each of the three groups:
  

```{r - Distribution of suffixes - Value column, message=FALSE, echo=FALSE, fig.align='center'}
#Fist checking the units of the values:
  dat_raw$Value %>% substr(nchar(dat_raw$Value), nchar(dat_raw$Value)) %>% table()
```


Besides this, a few non numerical values (na) had to be removed. Finally, the columns that were intuitively considered as meaningful for the modelling were extracted from the initial database and some column names were modified because contraining dots.

At this stage, a quick data visualization shows that the average values are considerably different from one column to the other (x-scale in log-2):


```{r-mean-predictors, echo=FALSE, fig.align='center'}
#Looking at the Average of each column
  dat_tuned_graph_avg <- dat_tuned %>% as.matrix() %>% colMeans() %>% as.data.frame()
  names(dat_tuned_graph_avg)[names(dat_tuned_graph_avg) == "."] <- "column_avg"
dat_tuned_graph_avg %>% ggplot(aes(row.names(dat_tuned_graph_avg),column_avg)) + geom_point() + scale_y_continuous(trans="log2") + xlab("predictors") + ylab("average") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("predictors average") + theme(plot.title = element_text(hjust=0.5))
```


Especially, the order of magnitude of "Value", "Wage"" and "Release_Clause" is much higher than for the other variables. "Value" is our output and should therefore not be modified. However, the other columns - the predictors - must be scaled. Note that to make it properly, it is necessary to first split the training and the test set as it is not allowed to use the test set in the scalling operation.

```{r preprocessing-data, message=FALSE, echo=FALSE, fig.align='center'}
   #Separating predictors and outcome
    y <- dat_tuned$Value
    x <- dat_tuned %>% select(-Value)
  #Defining training and test set
    test_index <- createDataPartition(y=dat_tuned$Value, times = 1,
                                      p=0.2, list = FALSE)
    train_set_x <- x[-test_index,]
    train_set_y <- y[-test_index]
    test_set_x <- x[test_index,]
    test_set_y <- y[test_index]
  #Scaling the predictors
    preProc_x <- preProcess(train_set_x, method = "scale")
    train_set_x_scaled <- predict(preProc_x, train_set_x)
    test_set_x_scaled <- predict(preProc_x, test_set_x)
  #Rebuilding the dataframes
    train_set <-dat_tuned[-test_index,]
    test_set <-dat_tuned[test_index,]
    train_set_scaled <- train_set_x_scaled %>% mutate(Value = train_set_y)
    test_set_scaled <- test_set_x_scaled %>% mutate(Value = test_set_y)
```

The scaled data is, as expected, much more balanced:


```{r-mean-predictors - scaled, echo=FALSE, fig.align='center'}
#Looking at the Average of each column
  dat_tuned_graph_avg_scaled <- train_set_x_scaled %>% as.matrix() %>% colMeans() %>% as.data.frame()
  names(dat_tuned_graph_avg_scaled)[names(dat_tuned_graph_avg_scaled) == "."] <- "column_avg"
  dat_tuned_graph_avg_scaled %>% ggplot(aes(row.names(dat_tuned_graph_avg_scaled),column_avg)) + geom_point() + xlab("predictors") + ylab("average") + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("predictors average after scaling") + theme(plot.title = element_text(hjust=0.5))
```


The data is now ready for the modelling. To test the error of our model, two metrics will be used, given by the following formulas:


```{r - Functions to calculate the errors, message=FALSE, fig.align='center'}
    #function RMSE
    RMSE <- function(true_values, predicted_values){
      sqrt(mean((true_values - predicted_values)^2))
    }
  #function WMAPE
    WMAPE <- function(true_values, predicted_values){
      sum(abs(true_values - predicted_values))/sum(true_values)
    }
```


In addition to the RMSE metric, it is decided to use another one, the WMAPE. Indeed, as the Value that we are trying to fit ranges from 0 to 100M, the RMSE that will result from the modelling are quite huge and therefore not so easy to interpretate. As the WMAPE metric is a ratio over the sum of the true values, it is much easier to interpretate.

We start with the naive approach of predicting the global average for all players.


```{r - Modeling global average, message=FALSE, echo=FALSE, warning=FALSE}
#Computating the global average to have a comparison basis
  Global_Average <- mean(train_set_y)
  RMSE_Global_Average <- RMSE(test_set_y, Global_Average)
  WMAPE_Global_Average <- WMAPE(test_set_y, Global_Average)
#Table to compare the results of all different approaches
  rmse_results <- data_frame(method = "Global Average" , RMSE = RMSE_Global_Average, WMAPE = WMAPE_Global_Average)
```

```{r - Modeling global average - to print, message=FALSE}
  RMSE_Global_Average
  WMAPE_Global_Average
```


We see that, as expected, the RMSE value is quite huge. The next obvious model to test is the linear model.


```{r - Modeling Lm, message=FALSE, echo=FALSE}
  #Lm model
    fit_lm <- lm(Value~., data =train_set_scaled)
    y_hat_Value <- predict(fit_lm, test_set_scaled)
    RMSE_lm <- RMSE(test_set_scaled$Value, y_hat_Value)
    WMAPE_lm <- WMAPE(test_set_scaled$Value, y_hat_Value)
    rmse_results <- bind_rows(rmse_results, data_frame(method="Lm model",
                                                       RMSE = RMSE_lm, WMAPE=WMAPE_lm))
```

```{r - Modeling Lm - to print, message=FALSE}
  RMSE_lm
  WMAPE_lm
```


This surely shows a great improvement over the obtained result with the global average. However, it seems clear that much better can be done. Running a logistic regression model leads to very similar results than the linear model:


```{r - Modeling glm, message=FALSE, echo=FALSE}
  #glm model
    train_glm <- train(Value~., method = "glm", data = train_set_scaled)
    y_hat_Value <- predict(train_glm, test_set_scaled, type="raw")
    RMSE_glm <- RMSE(test_set_scaled$Value, y_hat_Value)
    WMAPE_glm <- WMAPE(test_set_scaled$Value, y_hat_Value)
    rmse_results <- bind_rows(rmse_results, data_frame(method="glm model",
                                                       RMSE = RMSE_glm, WMAPE=WMAPE_glm))
```

```{r - Modeling glm - to print, message=FALSE}
  RMSE_glm
  WMAPE_glm
```


The next model that is tried is knn. In spite of much longer running time, the obtained result is worse than that of lm and glm.


```{r - Modeling knn, message=FALSE, echo=FALSE}
  #knn model
    train_knn <- train(Value~., method = "knn", data = train_set_scaled)
    y_hat_Value <- predict(train_knn, test_set_scaled, type="raw")
    RMSE_knn <- RMSE(test_set_scaled$Value, y_hat_Value)
    WMAPE_knn <- WMAPE(test_set_scaled$Value, y_hat_Value)
    rmse_results <- bind_rows(rmse_results, data_frame(method="knn model",
                                                       RMSE = RMSE_knn, WMAPE=WMAPE_knn))
```

```{r - Modeling knn - to print, message=FALSE}
  RMSE_knn
  WMAPE_knn
```


Attempts have been made to optimized k with cross-validation but this did not lead to any significant improvement, that's why this part is not shown is the report. The next model to be tried is rpart, fitting a single regression tree:


```{r - Modeling rpart, message=FALSE, echo=FALSE}
  #Rpart model
    fit_rpart <- rpart(Value~., data=train_set_scaled)
    y_hat_Value <- predict(fit_rpart, test_set_scaled)
    RMSE_rpart <- RMSE(test_set_scaled$Value, y_hat_Value)
    WMAPE_rpart <- WMAPE(test_set_scaled$Value, y_hat_Value)
    rmse_results <- bind_rows(rmse_results, data_frame(method="Rpart model",
                                                       RMSE = RMSE_rpart, WMAPE=WMAPE_rpart))
```


```{r - Modeling rpart - to print, message=FALSE}
  RMSE_rpart
  WMAPE_rpart
```


The result is even worse than that of lm, glm and knn. Optimzing cp did not help much which is why it is not presented here. However, as explained in the lecture "Machine Learning", averaging over a lot of trees shows always better results than having a single tree. This is why randomForest is the next model that is tried out:

```{r - Modeling randomForest, message=FALSE, echo=FALSE}
  #Random Forest model:
    fit_rf <- randomForest(Value~., data=train_set_scaled)
    y_hat_Value <- predict(fit_rf, test_set_scaled)
    RMSE_rf <- RMSE(test_set_scaled$Value, y_hat_Value)
    WMAPE_rf <- WMAPE(test_set_scaled$Value, y_hat_Value)
    rmse_results <- bind_rows(rmse_results, data_frame(method="RF model",
                                                       RMSE = RMSE_rf, WMAPE=WMAPE_rf))
```

```{r - Modeling randomForest - to print, message=FALSE}
  RMSE_rf
  WMAPE_rf
```


This time, the improvement is very significant. So far, this is the result to beat. The next model that is tested is xgbTree. We see that this model does about as well as randomForest:


```{r - Modeling xgbTree, message=FALSE, echo=FALSE}
#xgbTree model
  train_xgbTree <- train(Value~., method = "xgbTree", data = train_set_scaled, tuneGrid=expand.grid(nrounds = c(150,250), max_depth=3, eta=c(0.3,0.4), gamma=0, colsample_bytree = 0.8, min_child_weight=1, subsample=1))
    y_hat_Value <- predict(train_xgbTree, test_set_scaled, type="raw")
    RMSE_xgbTree <- RMSE(test_set_scaled$Value, y_hat_Value)
    WMAPE_xgbTree <- WMAPE(test_set_scaled$Value, y_hat_Value)
    rmse_results <- bind_rows(rmse_results, data_frame(method="xgbTree model",
                                                       RMSE = RMSE_xgbTree, WMAPE=WMAPE_xgbTree))
```

```{r - Modeling xgbTree - to print, message=FALSE}
    RMSE_xgbTree
    WMAPE_xgbTree
```


Two other models are tested: Gamboost & gamLoess. Many other algorithm were tested but they would not converge in a reasonabe time. Adding them to the table gives us the following results:


```{r - Gamboost & gamLoess, message=FALSE, echo=FALSE, warning=FALSE}

  #gamboost model
    train_gamboost <- train(Value~., method = "gamboost", data = train_set_scaled)
    y_hat_Value <- predict(train_gamboost, test_set_scaled, type="raw")
    RMSE_gamboost <- RMSE(test_set_scaled$Value, y_hat_Value)
    WMAPE_gamboost <- WMAPE(test_set_scaled$Value, y_hat_Value)
    rmse_results <- bind_rows(rmse_results, data_frame(method="gamboost model",
                                                       RMSE = RMSE_gamboost, WMAPE=WMAPE_gamboost))
    
  #gamLoess model
    train_gamLoess <- train(Value~., method = "gamLoess", data = train_set_scaled)
    y_hat_Value <- predict(train_gamLoess, test_set_scaled, type="raw")
    RMSE_gamLoess <- RMSE(test_set_scaled$Value, y_hat_Value)
    WMAPE_gamLoess <- WMAPE(test_set_scaled$Value, y_hat_Value)
    rmse_results <- bind_rows(rmse_results, data_frame(method="gamLoess model",
                                                       RMSE = RMSE_gamLoess, WMAPE=WMAPE_gamLoess))
```

```{r - Modeling gamboost & gamLoess - to print, message=FALSE}

  #gamboost model
    RMSE_gamboost
    WMAPE_gamboost
    
  #gamLoess model
    RMSE_gamLoess
    WMAPE_gamLoess
```


## Results Analysis and further optimization

The below table summarizes the so far obtained results with the different method explained above:

```{r - Result Summary, message=FALSE, echo=FALSE, fig.align='center'}
#Result summary
  rmse_results %>% knitr::kable()
```


Tunning mtry - the rf model could be furhter optimized but only slightly.


```{r - rf - best tunning , message=FALSE, echo=FALSE}
  #Random Forest model - Best tunning paramter
    fit_rf_mtry_20 <- randomForest(Value~., data=train_set_scaled, mtry=20)
    y_hat_Value_mtry_20 <- predict(fit_rf_mtry_20, test_set_scaled)
    RMSE_rf_mtry_20 <- RMSE(test_set_scaled$Value, y_hat_Value_mtry_20)
    WMAPE_rf_mtry_20 <- WMAPE(test_set_scaled$Value, y_hat_Value_mtry_20)
    rmse_results <- bind_rows(rmse_results, data_frame(method="RF best tune",
                                                       RMSE = RMSE_rf_mtry_20, WMAPE=WMAPE_rf_mtry_20))
```

```{r - rf - best tunning - to print, message=FALSE}
  #Random Forest model - Best tunning parameters
    RMSE_rf_mtry_20
    WMAPE_rf_mtry_20
```


Inspecting the training object of XgbTree, we see that the model is tested for 108 combinations of 7 tunning parameters. We try to optimize the parameters but a better result could not be obtained:


```{r - XgbTree - other tunning, message=FALSE, echo=FALSE}
#xgbTree model - other tunning parameters
  train_xgbTree_other <- train(Value~., method = "xgbTree", data = train_set_scaled,
                         tuneGrid=expand.grid(nrounds = 800, max_depth =3, eta=0.3, 
                                              gamma=0, colsample_bytree = 0.8, min_child_weight=1, subsample=1), 
                         trcontrol = trainControl(method = "cv", number = 5, p=0.8))
  y_hat_Value_other_xgbTree <- predict(train_xgbTree_other, test_set_scaled, type="raw")
  RMSE_xgbTree_other <- RMSE(test_set_scaled$Value, y_hat_Value)
  WMAPE_xgbTree_other <- WMAPE(test_set_scaled$Value, y_hat_Value)
  rmse_results <- bind_rows(rmse_results, data_frame(method="xgbTree other tune",
                                                       RMSE = RMSE_xgbTree_other, WMAPE=WMAPE_xgbTree_other))
```

```{r - XgbTree - other tunning - to print, message=FALSE}
  #xgbTree model - other tunning parameters
    RMSE_xgbTree_other
    WMAPE_xgbTree_other
```


Here is the final table result:

```{r - Result Summary - 3, message=FALSE, echo=FALSE, fig.align='center'}
#Result summary
  rmse_results %>% knitr::kable()
```


At this stage, it is relevant to look at the error of both models in order to see if one is better in a certain value "Area".


```{r - Plot - Absolute error of prediction vs. Value for RandomForest, message=FALSE, echo=FALSE , fig.align='center'}
#Replacing 0 by 1 for log plot
  Value_without_0_test <- test_set_scaled$Value
  Value_without_0_test[Value_without_0_test==0] <- 1
#Plots
    test_set_scaled %>% mutate(y_hat = y_hat_Value_mtry_20, absolute_error = abs(y_hat - Value), 
                   Value = Value_without_0_test) %>%  group_by(Value)%>%ggplot(aes(Value, absolute_error)) +        geom_point(color = "black") + ggtitle("Random Forest : Absolute Error vs Value")
```

```{r - Plot - Absolute error of prediction vs. Value for XgbTree , message=FALSE, echo=FALSE , fig.align='center'}
  test_set_scaled %>% mutate(y_hat = y_hat_Value_other_xgbTree, absolute_error = abs(y_hat - Value), 
                             Value = Value_without_0_test) %>%  group_by(Value)%>%ggplot(aes(Value, absolute_error)) + geom_point(color = "black") + ggtitle("xgbTree : Absolute Error vs Value")
```

```{r - Plot - Absolute error of prediction vs. Value for RandomForest - log scale, message=FALSE, echo=FALSE , fig.align='center'}
  test_set_scaled %>% mutate(y_hat = y_hat_Value_mtry_20, absolute_error = abs(y_hat - Value), 
                   Value = Value_without_0_test) %>%  group_by(Value)%>%ggplot(aes(Value, absolute_error)) +        geom_point(color = "black") + scale_x_continuous(trans="log2") + scale_y_continuous(trans="log2") +
    ggtitle("Random Forest : Absolute Error vs Value log scales") + theme(plot.title = element_text(hjust=0.5))
```

```{r - Plot - Absolute error of prediction vs. Value for XgbTree - log scale, message=FALSE, echo=FALSE , fig.align='center'}
  test_set_scaled %>% mutate(y_hat = y_hat_Value_other_xgbTree, absolute_error = abs(y_hat - Value), 
                             Value = Value_without_0_test) %>%  group_by(Value)%>%ggplot(aes(Value, absolute_error)) + geom_point(color = "black") + scale_x_continuous(trans="log2") + scale_y_continuous(trans="log2") +
    ggtitle("xgbTree : Absolute Error vs Value log scales") + theme(plot.title = element_text(hjust=0.5))
```


Looking at the above plots, we see that both models are doing quite well for low to average value, but they are both rather bad on extreme values, wether 0's or very large values. This is actually very normal as there are less players on the extreme and therefore less error to minimize. Looking at the top_10 errors of both models.

Top 10 error of RandomForest:


```{r - top 10 errors of RandomForest, message=FALSE, echo=FALSE , fig.align='center'}
test_set_scaled %>% mutate(y_hat = y_hat_Value_mtry_20, absolute_error = abs(y_hat - Value)) %>%  select(Value, absolute_error) %>% arrange(desc(Value)) %>% mutate(Value_Rank = seq(1, nrow(.), 1)) %>% arrange(desc(absolute_error)) %>% head(n=10)
```


Top 10 error of XgbTree:


```{r - top 10 errors of XgbTree, message=FALSE, echo=FALSE , fig.align='center'}
test_set_scaled %>% mutate(y_hat = y_hat_Value_other_xgbTree, absolute_error = abs(y_hat - Value)) %>%  select(Value, absolute_error) %>% arrange(desc(Value)) %>% mutate(Value_Rank = seq(1, nrow(.), 1)) %>% arrange(desc(absolute_error)) %>% head(n=10)
```


Here again, it is difficult to draw any conclusion on extreme errors. In both cases, we see most of the players with highest values appearing in the top 10.


## Conclusion

To conlude this project, it can be said that two of the models are showing fair results given the huge range of values of the output - the value of the players. As both models are doing better on one metric and none of them is really bad on one of the metric, we conclude that both are suitable for this database.

In order to improve to results further, several approaches are possible. Among other, more pre-processing could be done. Especially, some of the variables that have been excluded from the beginning could have been tested. (after being converted to dummy variables, for somme of them or after being wrangled) This is mostly a reason of time why this is not tried in this project. 

In the end, it is very likely that deep learning would have improved the results further as it usually shows better results than machine learning. This would be a next learning step as this is not covered at all in the Lecture.

